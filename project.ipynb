{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Files\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from csv import DictReader\n",
    "from efficient_apriori import apriori as eff_app\n",
    "import ast\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from itertools import chain\n",
    "import nltk\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Data Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to stem and tokenize the recipes\n",
    "def stem_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmed_items = []\n",
    "    for items in tokens:\n",
    "        stemmed_items.append(lemmatizer.lemmatize(items))\n",
    "    return stemmed_items\n",
    "\n",
    "def tokenize(text):    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the kaggle dataset\n",
    "# Create URL to JSON file (alternatively this can be a filepath)\n",
    "url = 'data/train.json'\n",
    "\n",
    "# Load the first sheet of the JSON file into a data frame\n",
    "df = pd.read_json(url, orient='columns')\n",
    "\n",
    "ingredients_1 = df['ingredients']\n",
    "# View the first ten rows\n",
    "train_list = ingredients_1.values.tolist()\n",
    "print(train_list[0:10])\n",
    "\n",
    "kaggle_cuisine_set = []\n",
    "for ingr in ingredients_1:\n",
    "    ingrStem = [' '.join(stem_tokens(tokenize(w))) for w in ingr]\n",
    "    kaggle_cuisine_set.append(ingrStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Schmidt Data Set\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "with np.load('data/simplified-recipes-1M.npz') as data:\n",
    "    recipes = data['recipes']\n",
    "    ingredients = data['ingredients']\n",
    "\n",
    "schmitd_set = [] #name this to recipe list\n",
    "#counter = 0\n",
    "for recipe in recipes:\n",
    "    if(recipe.size) == 0:\n",
    "        continue\n",
    "    schmitd_set.append(ingredients[recipe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'bad' ingredients such as 'sliced, prepared, etc'\n",
    "badingr = []\n",
    "\n",
    "file1 = open('data/baddata.txt', 'r') \n",
    "Lines = file1.readlines() \n",
    "for line in Lines: \n",
    "    badingr.append(line.strip())\n",
    "\n",
    "final_set_badwords= schmitd_set + kaggle_cuisine_set\n",
    "final_clean_data = []\n",
    "for recipe in final_set_badwords:\n",
    "    temprecipe = []\n",
    "    for ingr in recipe:\n",
    "        if ingr in badingr:\n",
    "            continue\n",
    "        temprecipe.append(ingr)\n",
    "    final_clean_data.append(temprecipe)\n",
    "\n",
    "print(len(final_clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing data into training and test sets\n",
    "\n",
    "final_training_data = [] #training set to run with apriori\n",
    "final_test_data = []     #represents ground truth\n",
    "\n",
    "for row in final_clean_data:   #each row is a recipe i.e list of ingredients\n",
    "    if len(row) == 0:\n",
    "        continue    \n",
    "    test_data = row.pop(random.randrange(len(row)))   #pops out random ingredients from the row\n",
    "    final_test_data.append(test_data)\n",
    "    final_training_data.append(row)\n",
    "    \n",
    "print(len(final_test_data))\n",
    "print(len(final_training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.  Association Rule Mining ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Appriori\n",
    "itemsets, rules = eff_app(final_training_data, min_support=0.08,  min_confidence=0.08)\n",
    "print(\"Rules =>\", rules)\n",
    "type(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle an object and save it to a file\n",
    "pickle.dump(rules, open(\"pickledRules\", \"wb\"))\n",
    "\n",
    "#reconstruct the pickled object\n",
    "reconstructedRules = pickle.load(open(\"pickledRules\", \"rb\"))\n",
    "print(type(reconstructedRules[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.  Calculating Precision ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing the rules\n",
    "testIngrList = ['salt']\n",
    "\n",
    "fileObj =  open(\"pickledRules\", \"rb\")\n",
    "newRules = pickle.load(fileObj)\n",
    "\n",
    "topKFinding = []\n",
    "\n",
    "#for testIngr in testIngrList:\n",
    "for r in newRules:\n",
    "    if set(testIngrList) == set(list(r.lhs)):\n",
    "        #Add Tuple of RHS and confidence to topKFinding\n",
    "        topKFinding.append((list(r.rhs), r.confidence))\n",
    "\n",
    "#Sort top k ingredients mapped from ARM in decreasing order of confidence\n",
    "topKFinding.sort(key = lambda x: -x[1]) \n",
    "\n",
    "k = 5\n",
    "\n",
    "topKFinding = topKFinding[0:k]\n",
    "print(topKFinding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_found = 0\n",
    "map_ingr = {}\n",
    "\n",
    "# Counts no of times an ingr is found in ground truth\n",
    "for top_ingr in topKFinding:\n",
    "    map_ingr[top_ingr[0][0]] = 0\n",
    "\n",
    "for recipe in final_training_data:    \n",
    "    if set(testIngrList).issubset(set(recipe)):\n",
    "        #Checks if ingredient found in ground truth is actually what we predicted\n",
    "        if final_test_data[index_found] in map_ingr:\n",
    "            #print(final_test_data[index_found])\n",
    "            #if yes then increase count            \n",
    "            map_ingr[final_test_data[index_found]] += 1\n",
    "    index_found += 1\n",
    "\n",
    "print(map_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calucating ARHR\n",
    "k = 1\n",
    "hits = 0\n",
    "summation = 0\n",
    "\n",
    "for top_ingr in topKFinding:\n",
    "    # if there was a hit for rank k-th item\n",
    "    if map_ingr[top_ingr[0][0]] > 0:     \n",
    "        summation += 1.0 / k \n",
    "        hits += 1\n",
    "    k += 1\n",
    "\n",
    "arhr = summation/hits\n",
    "print(arhr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calucating Average Precision\n",
    "k = 1\n",
    "hits = 0\n",
    "summation = 0\n",
    "\n",
    "for top_ingr in topKFinding:\n",
    "    # if there was a hit for rank k-th item\n",
    "    if map_ingr[top_ingr[0][0]] > 0:     \n",
    "        #print(hits + 1, k)\n",
    "        summation += (hits + 1) / k \n",
    "        hits += 1\n",
    "    k += 1\n",
    "\n",
    "ap = summation/hits\n",
    "print(ap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
